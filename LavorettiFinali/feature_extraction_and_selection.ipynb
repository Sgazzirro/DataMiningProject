{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Con questa cella qualsiasi file .py dentro creation_functions puà essere importato\n",
    "import sys\n",
    "sys.path.append(\"creation_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Ad esempio, posso importare la funzione \"create_time_series\"\n",
    "# SINTASSI : from creation_functions.<nomeFile.py> import <nomeFunzione>\n",
    "from creation_functions.utilities import create_time_series"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Descrizione per Marco del file utilities.py e feature_extraction.py\n",
    "in utilities.py troverai due funzioni:\n",
    "- _create_time_series_ fa tutto il merge e restituisce il dataset con delle colonne carine \"subject\" e \"trial\" per identificare attori e tentativo. Se la chiami senza argomenti restituisce il dataset collassato, se la chiami con mode=\"raw\" restituisce il dataset originale (appunto con quelle colonne in più)\n",
    "- _get_some_filter_ è una funzione per filtrare il dataset, che sia collassato o meno, sulla base di attori è attività. Prende come argomento il dataset, una lista di attori e una lista di attività, e restituisce i dati di solo quegli attori e solo quelle attività\n",
    "\n",
    "In feature_extraction.py troverai tutte le funzioni necessarie alla creazione del dataset collassato"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Possibili linee guida del notebook\n",
    "- Le funzioni di _feature_extraction.py_ sono un punto d'arrivo. Avrebbe senso riportare qui alcune di quelle funzioni, come quelle della trasformata di fourier, e testare un po' di parametri (come l'N di segmentazione). In feature_extraction.py poi si riporterebbo direttamente i parametri scelti.\n",
    "\n",
    "1. Possibili ragionamenti sullo scaling dei dati?\n",
    "2. Parlare dei parametri scelti per la FFT. In particolare dire che la frequenza è quella del dataset, e se trovi un immaginetta carina della FFT (intesa come equazione) mettercela non fa male\n",
    "2. Mostrare le variazioni nel tempo e in frequenza a seconda dell'attività considerata\n",
    "3. Far vedere magari che alcune attività hanno pochi picchi significativi e che quindi \"in media\", si è pensato che 5 picchi potesse essere un buon numero\n",
    "4. O magari il ragionamento di quanti picchi trovare potrebbe essere anche una cosa di \"quanto tempo ci mette\", e quindi scegliere un numero di picchi tale che, se dovesse trovarne di più, ci metterebbe troppo tempo\n",
    "5. Una volta parlato delle feature estratte, far vedere che non tutte sono rilevanti, magari plottando userAcceleration.y_std e userAcceleration.y_range che sappiamo essere correlate al 99%. Un processo di feature selection si rende quindi necessario\n",
    "6. Da qui in poi cominciare lo studio di quale K selezionare per SelectKBest (dopo aver in qualche modo caricato il dataset preprocessato con le feature ridondanti eliminate)\n",
    "7. Svolgere lo studio magari in parallelo considerando sia la grandezza delle finestre sia K\n",
    "8. Far capire che quindi l'obiettivo del notebook è stato quello di decidere in maniera non troppo stupida sia il numero di picchi da trovare, sia N, sia K"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
